{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/santhosr/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "import time\n",
    "import string\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import filterfalse\n",
    "from functools import reduce\n",
    "from scipy import sparse\n",
    "\n",
    "import gc\n",
    "\n",
    "from dask import distributed\n",
    "from dask.distributed import Client, LocalCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### GLoVe Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from collections import Counter, defaultdict\n",
    "import os\n",
    "from random import shuffle\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class NotTrainedError(Exception):\n",
    "    pass\n",
    "\n",
    "class NotFitToCorpusError(Exception):\n",
    "    pass\n",
    "\n",
    "class GloVeModel():\n",
    "    def __init__(self, embedding_size, context_size, max_vocab_size=100000, min_occurrences=1,\n",
    "                 scaling_factor=3/4, cooccurrence_cap=100, batch_size=512, learning_rate=0.05):\n",
    "        self.embedding_size = embedding_size\n",
    "        if isinstance(context_size, tuple):\n",
    "            self.left_context, self.right_context = context_size\n",
    "        elif isinstance(context_size, int):\n",
    "            self.left_context = self.right_context = context_size\n",
    "        else:\n",
    "            raise ValueError(\"`context_size` should be an int or a tuple of two ints\")\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.min_occurrences = min_occurrences\n",
    "        self.scaling_factor = scaling_factor\n",
    "        self.cooccurrence_cap = cooccurrence_cap\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.__words = None\n",
    "        self.__word_to_id = None\n",
    "        self.__cooccurrence_matrix = None\n",
    "        self.__embeddings = None\n",
    "\n",
    "    def fit_to_corpus(self, corpus):\n",
    "        self.__fit_to_corpus(corpus, self.max_vocab_size, self.min_occurrences,\n",
    "                             self.left_context, self.right_context)\n",
    "        self.__build_graph()\n",
    "\n",
    "    def __fit_to_corpus(self, corpus, vocab_size, min_occurrences, left_size, right_size):\n",
    "        word_counts = Counter()\n",
    "        cooccurrence_counts = defaultdict(float)\n",
    "        for region in tqdm(corpus):\n",
    "            word_counts.update(region)\n",
    "            for l_context, word, r_context in _context_windows(region, left_size, right_size):\n",
    "                for i, context_word in enumerate(l_context[::-1]):\n",
    "                    # add (1 / distance from focal word) for this pair\n",
    "                    cooccurrence_counts[(word, context_word)] += 1 / (i + 1)\n",
    "                for i, context_word in enumerate(r_context):\n",
    "                    cooccurrence_counts[(word, context_word)] += 1 / (i + 1)\n",
    "        if len(cooccurrence_counts) == 0:\n",
    "            raise ValueError(\"No coccurrences in corpus. Did you try to reuse a generator?\")\n",
    "        self.__words = [word for word, count in word_counts.most_common(vocab_size)\n",
    "                        if count >= min_occurrences]\n",
    "        self.__word_to_id = {word: i for i, word in enumerate(self.__words)}\n",
    "        self.__cooccurrence_matrix = {\n",
    "            (self.__word_to_id[words[0]], self.__word_to_id[words[1]]): count\n",
    "            for words, count in cooccurrence_counts.items()\n",
    "            if words[0] in self.__word_to_id and words[1] in self.__word_to_id}\n",
    "        \n",
    "        \n",
    "    def fit_to_cmatrix(self, cmatrix, vocab):\n",
    "        \n",
    "\n",
    "    def __build_graph(self):\n",
    "        self.__graph = tf.Graph()\n",
    "        with self.__graph.as_default(), self.__graph.device(_device_for_node):\n",
    "            count_max = tf.constant([self.cooccurrence_cap], dtype=tf.float32,\n",
    "                                    name='max_cooccurrence_count')\n",
    "            scaling_factor = tf.constant([self.scaling_factor], dtype=tf.float32,\n",
    "                                         name=\"scaling_factor\")\n",
    "\n",
    "            self.__focal_input = tf.placeholder(tf.int32, shape=[self.batch_size],\n",
    "                                                name=\"focal_words\")\n",
    "            self.__context_input = tf.placeholder(tf.int32, shape=[self.batch_size],\n",
    "                                                  name=\"context_words\")\n",
    "            self.__cooccurrence_count = tf.placeholder(tf.float32, shape=[self.batch_size],\n",
    "                                                       name=\"cooccurrence_count\")\n",
    "\n",
    "            focal_embeddings = tf.Variable(\n",
    "                tf.random_uniform([self.vocab_size, self.embedding_size], 1.0, -1.0),\n",
    "                name=\"focal_embeddings\")\n",
    "            context_embeddings = tf.Variable(\n",
    "                tf.random_uniform([self.vocab_size, self.embedding_size], 1.0, -1.0),\n",
    "                name=\"context_embeddings\")\n",
    "\n",
    "            focal_biases = tf.Variable(tf.random_uniform([self.vocab_size], 1.0, -1.0),\n",
    "                                       name='focal_biases')\n",
    "            context_biases = tf.Variable(tf.random_uniform([self.vocab_size], 1.0, -1.0),\n",
    "                                         name=\"context_biases\")\n",
    "\n",
    "            focal_embedding = tf.nn.embedding_lookup([focal_embeddings], self.__focal_input)\n",
    "            context_embedding = tf.nn.embedding_lookup([context_embeddings], self.__context_input)\n",
    "            focal_bias = tf.nn.embedding_lookup([focal_biases], self.__focal_input)\n",
    "            context_bias = tf.nn.embedding_lookup([context_biases], self.__context_input)\n",
    "\n",
    "            weighting_factor = tf.minimum(\n",
    "                1.0,\n",
    "                tf.pow(\n",
    "                    tf.div(self.__cooccurrence_count, count_max),\n",
    "                    scaling_factor))\n",
    "\n",
    "            embedding_product = tf.reduce_sum(tf.multiply(focal_embedding, context_embedding), 1)\n",
    "\n",
    "            log_cooccurrences = tf.log(tf.to_float(self.__cooccurrence_count))\n",
    "\n",
    "            distance_expr = tf.square(tf.add_n([\n",
    "                embedding_product,\n",
    "                focal_bias,\n",
    "                context_bias,\n",
    "                tf.negative(log_cooccurrences)]))\n",
    "\n",
    "            single_losses = tf.multiply(weighting_factor, distance_expr)\n",
    "            self.__total_loss = tf.reduce_sum(single_losses)\n",
    "            tf.summary.scalar(\"GloVe_loss\", self.__total_loss)\n",
    "            self.__optimizer = tf.train.AdagradOptimizer(self.learning_rate).minimize(\n",
    "                self.__total_loss)\n",
    "            self.__summary = tf.summary.merge_all()\n",
    "\n",
    "            self.__combined_embeddings = tf.add(focal_embeddings, context_embeddings,\n",
    "                                                name=\"combined_embeddings\")\n",
    "\n",
    "    def train(self, num_epochs, log_dir=None, summary_batch_interval=1000,\n",
    "              tsne_epoch_interval=None):\n",
    "        should_write_summaries = log_dir is not None and summary_batch_interval\n",
    "        should_generate_tsne = log_dir is not None and tsne_epoch_interval\n",
    "        batches = self.__prepare_batches()\n",
    "        total_steps = 0\n",
    "        with tf.Session(graph=self.__graph) as session:\n",
    "            if should_write_summaries:\n",
    "                print(\"Writing TensorBoard summaries to {}\".format(log_dir))\n",
    "                summary_writer = tf.summary.FileWriter(log_dir, graph=session.graph)\n",
    "            tf.global_variables_initializer().run()\n",
    "            for epoch in range(num_epochs):\n",
    "                shuffle(batches)\n",
    "                for batch_index, batch in enumerate(batches):\n",
    "                    i_s, j_s, counts = batch\n",
    "                    if len(counts) != self.batch_size:\n",
    "                        continue\n",
    "                    feed_dict = {\n",
    "                        self.__focal_input: i_s,\n",
    "                        self.__context_input: j_s,\n",
    "                        self.__cooccurrence_count: counts}\n",
    "                    session.run([self.__optimizer], feed_dict=feed_dict)\n",
    "                    if should_write_summaries and (total_steps + 1) % summary_batch_interval == 0:\n",
    "                        summary_str = session.run(self.__summary, feed_dict=feed_dict)\n",
    "                        summary_writer.add_summary(summary_str, total_steps)\n",
    "                    total_steps += 1\n",
    "                if should_generate_tsne and (epoch + 1) % tsne_epoch_interval == 0:\n",
    "                    current_embeddings = self.__combined_embeddings.eval()\n",
    "                    output_path = os.path.join(log_dir, \"epoch{:03d}.png\".format(epoch + 1))\n",
    "                    self.generate_tsne(output_path, embeddings=current_embeddings)\n",
    "            self.__embeddings = self.__combined_embeddings.eval()\n",
    "            if should_write_summaries:\n",
    "                summary_writer.close()\n",
    "\n",
    "    def embedding_for(self, word_str_or_id):\n",
    "        if isinstance(word_str_or_id, str):\n",
    "            return self.embeddings[self.__word_to_id[word_str_or_id]]\n",
    "        elif isinstance(word_str_or_id, int):\n",
    "            return self.embeddings[word_str_or_id]\n",
    "\n",
    "    def __prepare_batches(self):\n",
    "        if self.__cooccurrence_matrix is None:\n",
    "            raise NotFitToCorpusError(\n",
    "                \"Need to fit model to corpus before preparing training batches.\")\n",
    "        cooccurrences = [(word_ids[0], word_ids[1], count)\n",
    "                         for word_ids, count in self.__cooccurrence_matrix.items()]\n",
    "        i_indices, j_indices, counts = zip(*cooccurrences)\n",
    "        return list(_batchify(self.batch_size, i_indices, j_indices, counts))\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.__words)\n",
    "\n",
    "    @property\n",
    "    def words(self):\n",
    "        if self.__words is None:\n",
    "            raise NotFitToCorpusError(\"Need to fit model to corpus before accessing words.\")\n",
    "        return self.__words\n",
    "\n",
    "    @property\n",
    "    def embeddings(self):\n",
    "        if self.__embeddings is None:\n",
    "            raise NotTrainedError(\"Need to train model before accessing embeddings\")\n",
    "        return self.__embeddings\n",
    "\n",
    "    def id_for_word(self, word):\n",
    "        if self.__word_to_id is None:\n",
    "            raise NotFitToCorpusError(\"Need to fit model to corpus before looking up word ids.\")\n",
    "        return self.__word_to_id[word]\n",
    "\n",
    "    def generate_tsne(self, path=None, size=(100, 100), word_count=1000, embeddings=None):\n",
    "        if embeddings is None:\n",
    "            embeddings = self.embeddings\n",
    "        from sklearn.manifold import TSNE\n",
    "        tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "        low_dim_embs = tsne.fit_transform(embeddings[:word_count, :])\n",
    "        labels = self.words[:word_count]\n",
    "        return _plot_with_labels(low_dim_embs, labels, path, size)\n",
    "\n",
    "\n",
    "def _context_windows(region, left_size, right_size):\n",
    "    for i, word in enumerate(region):\n",
    "        start_index = i - left_size\n",
    "        end_index = i + right_size\n",
    "        left_context = _window(region, start_index, i - 1)\n",
    "        right_context = _window(region, i + 1, end_index)\n",
    "        yield (left_context, word, right_context)\n",
    "\n",
    "\n",
    "def _window(region, start_index, end_index):\n",
    "    \"\"\"\n",
    "    Returns the list of words starting from `start_index`, going to `end_index`\n",
    "    taken from region. If `start_index` is a negative number, or if `end_index`\n",
    "    is greater than the index of the last word in region, this function will pad\n",
    "    its return value with `NULL_WORD`.\n",
    "    \"\"\"\n",
    "    last_index = len(region) + 1\n",
    "    selected_tokens = region[max(start_index, 0):min(end_index, last_index) + 1]\n",
    "    return selected_tokens\n",
    "\n",
    "\n",
    "def _device_for_node(n):\n",
    "    if n.type == \"MatMul\":\n",
    "        return \"/gpu:0\"\n",
    "    else:\n",
    "        return \"/cpu:0\"\n",
    "\n",
    "\n",
    "def _batchify(batch_size, *sequences):\n",
    "    for i in range(0, len(sequences[0]), batch_size):\n",
    "        yield tuple(sequence[i:i+batch_size] for sequence in sequences)\n",
    "\n",
    "\n",
    "def _plot_with_labels(low_dim_embs, labels, path, size):\n",
    "    import matplotlib.pyplot as plt\n",
    "    assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
    "    figure = plt.figure(figsize=size)  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right',\n",
    "                     va='bottom')\n",
    "    if path is not None:\n",
    "        figure.savefig(path)\n",
    "        plt.close(figure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('/home/santhosr/Documents/Courses/GloVe/wikiData/WestburyLab.Wikipedia.Corpus.txt','r')\n",
    "\n",
    "dataLines = []\n",
    "\n",
    "for i in f:\n",
    "    dataLines.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30749930"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataLinesnes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanLine(x):\n",
    "    \n",
    "    x = x.replace('\\n','')\n",
    "    \n",
    "    x = x.lower()\n",
    "    punctuationString = string.punctuation\n",
    "    x = x.translate(str.maketrans(' ',' ', punctuationString))\n",
    "    \n",
    "    x = word_tokenize(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def processBatch(x):\n",
    "    x = [cleanLine(i) for i in x ]\n",
    "    \n",
    "    #Removing all docs with length < 2\n",
    "    x[:] = [doc for doc in x if len(doc)>=2]\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = LocalCluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:33321\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>67.19 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:33321' processes=4 cores=8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santhosr/.conda/envs/fastai/lib/python3.6/site-packages/distributed/worker.py:3101: UserWarning: Large object of size 51.53 MB detected in task graph: \n",
      "  (['\\n', 'Anarchism.\\n', 'Anarchism is a political  ...  to fade.\\n'],)\n",
      "Consider scattering large objects ahead of time\n",
      "with client.scatter to reduce scheduler burden and \n",
      "keep data on workers\n",
      "\n",
      "    future = client.submit(func, big_data)    # bad\n",
      "\n",
      "    big_future = client.scatter(big_data)     # good\n",
      "    future = client.submit(func, big_future)  # good\n",
      "  % (format_bytes(len(b)), s)\n",
      "distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://127.0.0.1:39826 remote=tcp://127.0.0.1:33321>\n",
      "distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://127.0.0.1:39828 remote=tcp://127.0.0.1:33321>\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    \n",
    "    a = client.map(processBatch, [dataLines[153740*j: 153740*(j+1)] for j in range(20*i,20*(i+1))] )\n",
    "    b= client.gather(a)\n",
    "    b = list(itertools.chain.from_iterable(b))\n",
    "    \n",
    "    with open('tokenizedData_'+str(i),'wb') as f:\n",
    "        pickle.dump(b,f)\n",
    "    \n",
    "    client.cancel(a)\n",
    "    client.cancel(b)\n",
    "    del a\n",
    "    del b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30749930"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataLines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = processBatch(dataLines[30748000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizedData_11','wb') as f:\n",
    "    pickle.dump(b,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pooled rpc to 'tcp://127.0.0.1:37921'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "####  Cleaning Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def cleanTokenFile(i):\n",
    "    \n",
    "    fileName = 'tokenizedData_'+str(i)\n",
    "    x = pickle.load(open(fileName,'rb'))\n",
    "    startSize = len(x)\n",
    "    \n",
    "    \n",
    "    #Removing all documents with 1 or less words in them\n",
    "    x[:] = [doc for doc in x if len(x)>=2]\n",
    "    \n",
    "    \n",
    "    endSize = len(x)\n",
    "    \n",
    "    print(\"Dropped {} items\".format(startSize - endSize))\n",
    "    \n",
    "    with open('tokenizedData_'+str(i),'wb') as f:\n",
    "        pickle.dump(x,f)\n",
    "    \n",
    "    del x\n",
    "    \n",
    "    return 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-6b4de4419fe6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleanTokenFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mendTime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fastai/lib/python3.6/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(self, futures, errors, maxsize, direct, asynchronous)\u001b[0m\n\u001b[1;32m   1820\u001b[0m                 \u001b[0mdirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m                 \u001b[0mlocal_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1822\u001b[0;31m                 \u001b[0masynchronous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1823\u001b[0m             )\n\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fastai/lib/python3.6/site-packages/distributed/client.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    751\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fastai/lib/python3.6/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m         \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fastai/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fastai/lib/python3.6/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                 \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fastai/lib/python3.6/site-packages/tornado/gen.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m                         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhad_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fastai/lib/python3.6/site-packages/tornado/gen.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1139\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mexc_info\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m                             \u001b[0myielded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m                         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m                             \u001b[0;31m# Break up a reference to itself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fastai/lib/python3.6/site-packages/distributed/client.py\u001b[0m in \u001b[0;36m_gather\u001b[0;34m(self, futures, errors, direct, local_worker)\u001b[0m\n\u001b[1;32m   1651\u001b[0m                             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCancelledError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1653\u001b[0;31m                             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1654\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"skip\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m                         \u001b[0mbad_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fastai/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    690\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-90-4776d2434af5>\u001b[0m in \u001b[0;36mcleanTokenFile\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfileName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tokenizedData_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mstartSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Worker process 1050 was killed by unknown signal\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Worker process 1055 was killed by unknown signal\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Worker process 1058 was killed by unknown signal\n",
      "distributed.nanny - WARNING - Restarting worker\n"
     ]
    }
   ],
   "source": [
    "iterations = 1\n",
    "startTime = time.time()\n",
    "cpuStart = time.clock()\n",
    "\n",
    "\n",
    "a = client.map(cleanTokenFile, list(range(1,11)))\n",
    "b = client.gather(a)\n",
    "\n",
    "endTime = time.time()\n",
    "cpuEnd = time.clock()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Iterations : {} Avg. Clock time : {} Avg. CPU time : {} Clock time : {}  CPU time : {}\"\\\n",
    "      .format(iterations, np.round((endTime - startTime)/iterations,4) \\\n",
    "    , np.round((cpuEnd - cpuStart)/iterations, 4), np.round(endTime - startTime,4), np.round(cpuEnd - cpuStart,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:33811\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>67.19 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:33811' processes=4 cores=8>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Creating Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getVocab(i):\n",
    "    \n",
    "    fileName = 'tokenizedData_'+str(i)\n",
    "    x = pickle.load(open(fileName,'rb'))\n",
    "    \n",
    "    counter = Counter()\n",
    "    for i in x:\n",
    "        counter.update(i)\n",
    "        \n",
    "    gc.collect()\n",
    "    \n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "client = Client()\n",
    "\n",
    "# client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations : 1 Avg. Clock time : 170.975 Avg. CPU time : 14.5834 Clock time : 170.975  CPU time : 14.5834\n"
     ]
    }
   ],
   "source": [
    "iterations = 1\n",
    "startTime = time.time()\n",
    "cpuStart = time.clock()\n",
    "\n",
    "\n",
    "a = client.map(getVocab, list(range(10)))\n",
    "b = client.gather(a)\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "cpuEnd = time.clock()\n",
    "\n",
    "\n",
    "print(\"Iterations : {} Avg. Clock time : {} Avg. CPU time : {} Clock time : {}  CPU time : {}\".format(iterations, np.round((endTime - startTime)/iterations, 4) , np.round((cpuEnd - cpuStart)/iterations, 4), np.round(endTime - startTime,4), np.round(cpuEnd - cpuStart,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "c = b[0]\n",
    "\n",
    "for i in range(1,len(b)):\n",
    "    c.update(b[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('wordCount','wb') as f:\n",
    "    pickle.dump(c,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Modifying Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = pickle.load(open('tokenizedData_0','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vocabSize = 100000\n",
    "wordCount = pickle.load(open('wordCount','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vocab = wordCount.most_common(vocabSize)\n",
    "\n",
    "## Creating the Word-ID dictionaries\n",
    "id_to_word = {i:x[0] for i,x in enumerate(vocab)}\n",
    "\n",
    "word_to_id = {value:key for key,value in id_to_word.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Loading Token Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def modifyToken(x):\n",
    "    \n",
    "    for doc in x:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:45079\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>67.19 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:45079' processes=4 cores=8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Broadcasting word_to_id to all workers\n",
    "_ = client.scatter(word_to_id, broadcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fileName = 'tokenizedData_'+str(0)\n",
    "x = pickle.load(open(fileName,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus = [x[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def createCMatrix(corpus):\n",
    "    \n",
    "    windowSize = 10\n",
    "\n",
    "    cooccurrences = sparse.lil_matrix((vocabSize, vocabSize), dtype=np.float64)\n",
    "\n",
    "    for doc in corpus:\n",
    "\n",
    "        for center_index, center_word in enumerate(doc):\n",
    "            \n",
    "            if center_word not in word_to_id.keys():\n",
    "                continue\n",
    "            \n",
    "            context = doc[max(0, center_index - windowSize) : center_index]\n",
    "            contextLen = len(context)\n",
    "            \n",
    "            \n",
    "\n",
    "            for context_index, context_word in enumerate(context):\n",
    "\n",
    "                dist = contextLen - context_index\n",
    "\n",
    "                inc = 1.0/float(dist)\n",
    "                \n",
    "                if context_word in word_to_id.keys():\n",
    "                    cooccurrences[word_to_id[center_word] , word_to_id[context_word]] += inc\n",
    "                    cooccurrences[word_to_id[context_word] , word_to_id[center_word]] += inc\n",
    "                \n",
    "    \n",
    "    return cooccurrences\n",
    "            \n",
    "            \n",
    "def split(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "corpus = pickle.load(open('tokenizedData_'+str(0),'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:45079\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>67.19 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:45079' processes=4 cores=8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [12:02, 722.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [24:03, 722.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    \n",
    "#     corpus = pickle.load(open('tokenizedData_'+str(i),'rb'))\n",
    "    \n",
    "    matrices = []\n",
    "    \n",
    "    \n",
    "    for sub in tqdm(split(corpus,10)):\n",
    "        \n",
    "        print(\"Start\")\n",
    "    \n",
    "        a = client.map(createCMatrix, list(split(sub,16)))\n",
    "        b = client.gather(a)\n",
    "        \n",
    "        mat = reduce(lambda x,y : x+y, b)\n",
    "        \n",
    "        matrices.append(mat.copy())\n",
    "        \n",
    "        client.cancel(a)\n",
    "        client.cancel(b)\n",
    "        \n",
    "        del a\n",
    "        del b\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Worker process 5980 was killed by unknown signal\n",
      "distributed.nanny - WARNING - Restarting worker\n"
     ]
    }
   ],
   "source": [
    "type(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "type(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 103733/2484555 [11:18<3:50:38, 172.04it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-c3235d7b7bb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateCMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-62-1805531170e7>\u001b[0m in \u001b[0;36mcreateCMatrix\u001b[0;34m(corpus, windowSize)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcontext_word\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_to_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                     \u001b[0mcooccurrences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_to_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcenter_word\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mword_to_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcontext_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0minc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                     \u001b[0mcooccurrences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_to_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcontext_word\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mword_to_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcenter_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0minc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "e = createCMatrix(corpus,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2484555"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "r= client.map(createCMatrix, [ [x[0]] , [x[1]]  ], [10,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "e= client.gather(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5, 3], [7, 8], [4, 6], [5, 4]]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(split([1,5,3,7,8,4,6,5,4],4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<100000x100000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1225 stored elements in LInked List format>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<100000x100000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1258 stored elements in LInked List format>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "windowSize = 10\n",
    "\n",
    "cooccurrences = sparse.lil_matrix((vocabSize, vocabSize), dtype=np.float64)\n",
    "\n",
    "for doc in corpus:\n",
    "\n",
    "    for center_index, center_word in enumerate(doc):\n",
    "        \n",
    "        context = doc[max(0, center_index - windowSize) : center_index]\n",
    "        contextLen = len(context)\n",
    "        \n",
    "        for context_index, context_word in enumerate(context):\n",
    "            \n",
    "            dist = contextLen - context_index\n",
    "            \n",
    "            inc = 1.0/float(dist)\n",
    "            \n",
    "            cooccurrences[word_to_id[center_word] , word_to_id[context_word]] += inc\n",
    "            cooccurrences[word_to_id[context_word] , word_to_id[center_word]] += inc\n",
    "            \n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cooccurrences[word_to_id['anarchism'], word_to_id['philosophy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 anarchism\n",
      "1 is\n",
      "2 a\n",
      "3 political\n",
      "4 philosophy\n",
      "5 which\n",
      "6 considers\n",
      "7 the\n",
      "8 state\n",
      "9 undesirable\n",
      "10 unnecessary\n",
      "11 and\n",
      "12 harmful\n",
      "13 and\n",
      "14 instead\n",
      "15 promotes\n",
      "16 a\n",
      "17 stateless\n",
      "18 society\n",
      "19 or\n",
      "20 anarchy\n",
      "21 it\n",
      "22 seeks\n",
      "23 to\n",
      "24 diminish\n",
      "25 or\n",
      "26 even\n",
      "27 abolish\n",
      "28 authority\n",
      "29 in\n",
      "30 the\n",
      "31 conduct\n",
      "32 of\n",
      "33 human\n",
      "34 relations\n",
      "35 anarchists\n",
      "36 may\n",
      "37 widely\n",
      "38 disagree\n",
      "39 on\n",
      "40 what\n",
      "41 additional\n",
      "42 criteria\n",
      "43 are\n",
      "44 required\n",
      "45 in\n",
      "46 anarchism\n",
      "47 the\n",
      "48 oxford\n",
      "49 companion\n",
      "50 to\n",
      "51 philosophy\n",
      "52 says\n",
      "53 there\n",
      "54 is\n",
      "55 no\n",
      "56 single\n",
      "57 defining\n",
      "58 position\n",
      "59 that\n",
      "60 all\n",
      "61 anarchists\n",
      "62 hold\n",
      "63 and\n",
      "64 those\n",
      "65 considered\n",
      "66 anarchists\n",
      "67 at\n",
      "68 best\n",
      "69 share\n",
      "70 a\n",
      "71 certain\n",
      "72 family\n",
      "73 resemblance\n"
     ]
    }
   ],
   "source": [
    "for i,j in enumerate(x[0]):\n",
    "    print(i,j)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Cooccur Matrix using Glove_python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b= pickle.load(open('wikiData/tokenizedData_4','rb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Glove_Cython\n",
    "import os\n",
    "currDir = os.getcwd()\n",
    "os.chdir(\"/home/santhosr/Documents/Courses/GloVe/glove_cython/Glove_Cython\")\n",
    "from glove_python import *\n",
    "os.chdir(currDir)\n",
    "\n",
    "## Importing Dictionary\n",
    "vocabSize = 1000000\n",
    "\n",
    "wordCount = pickle.load(open('wordCount','rb'))\n",
    "\n",
    "\n",
    "vocab = wordCount.most_common(vocabSize)\n",
    "\n",
    "id_to_word = {i:x[0] for i,x in enumerate(vocab)}\n",
    "\n",
    "word_to_id = {value:key for key,value in id_to_word.items()}\n",
    "\n",
    "c = Corpus(word_to_id)\n",
    "\n",
    "\n",
    "%timeit -n 1 -r 1 c.fit(b, ignore_missing=True)\n",
    "\n",
    "\n",
    "mat = c.matrix.tocsr()\n",
    "\n",
    "\n",
    "with open('coo_4','wb') as f:\n",
    "    pickle.dump(mat,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Cooccur Matrix ( Glove_Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pickle.load(open('/home/santhosr/Documents/Courses/GloVe/coo_0','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,10):\n",
    "    b = pickle.load(open('/home/santhosr/Documents/Courses/GloVe/coo_'+str(i),'rb'))\n",
    "    \n",
    "    a = a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('coo_full','wb') as f:\n",
    "    pickle.dump(a,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Glove_Cython\n",
    "import os\n",
    "currDir = os.getcwd()\n",
    "os.chdir(\"/home/santhosr/Documents/Courses/GloVe/glove_cython/Glove_Cython\")\n",
    "from glove_python import *\n",
    "os.chdir(currDir)\n",
    "\n",
    "## Importing Dictionary\n",
    "vocabSize = 1000000\n",
    "\n",
    "wordCount = pickle.load(open('wordCount','rb'))\n",
    "\n",
    "\n",
    "vocab = wordCount.most_common(vocabSize)\n",
    "\n",
    "id_to_word = {i:x[0] for i,x in enumerate(vocab)}\n",
    "\n",
    "word_to_id = {value:key for key,value in id_to_word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Corpus(word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.matrix = a.tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = Glove(no_components=200, learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove.add_dictionary(c.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 2 training epochs with 8 threads\n",
      "Epoch 0\n"
     ]
    }
   ],
   "source": [
    "glove.fit(c.matrix, epochs=2,\n",
    "          no_threads=8, verbose=True, wordList = ['board','approved','reuters','food','market','mutual','bond','cash','flow','money','directors'],save_gap=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Cooccur Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pickle.load(open('cooccurMat_0','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pickle.load(open('cooccurMat_1','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000000x1000000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 248425878 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000000x1000000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 246874559 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = a.todok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1218940.8619047815"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [(1,2),(3,4),(5,6),(7,8)]\n",
    "\n",
    "i,j = zip(*a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 5, 7)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Glove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = GloVeModel(embedding_size=100, context_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 962685/3074800 [11:13<23:51, 1475.49it/s] "
     ]
    }
   ],
   "source": [
    "glove.fit_to_corpus(itemlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
